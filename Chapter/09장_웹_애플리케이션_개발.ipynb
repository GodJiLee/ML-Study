{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9장 웹 애플리케이션 개발.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPE2P5PbaRP3zV1T9T3Kx5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GodJiLee/Machine_learning_study/blob/master/9%EC%9E%A5_%EC%9B%B9_%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98_%EA%B0%9C%EB%B0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnFwhVzNdRCA",
        "colab_type": "text"
      },
      "source": [
        "# 9장 웹 애플리케이션에 머신러닝 모델 내장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgmgrluBzKPw",
        "colab_type": "text"
      },
      "source": [
        "## 8장 복습하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhH4lejwiDA0",
        "colab_type": "code",
        "outputId": "8d9b8ba6-1154-4530-f735-d29ec9540d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "!wget https://github.com/rickiepark/python-machine-learning-book-2nd-edition/raw/master/code/ch09/movie_data.csv.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-08 05:01:55--  https://github.com/rickiepark/python-machine-learning-book-2nd-edition/raw/master/code/ch09/movie_data.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/rickiepark/python-machine-learning-book-2nd-edition/master/code/ch09/movie_data.csv.gz [following]\n",
            "--2020-06-08 05:01:55--  https://raw.githubusercontent.com/rickiepark/python-machine-learning-book-2nd-edition/master/code/ch09/movie_data.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26521894 (25M) [application/octet-stream]\n",
            "Saving to: ‘movie_data.csv.gz’\n",
            "\n",
            "movie_data.csv.gz   100%[===================>]  25.29M  35.2MB/s    in 0.7s    \n",
            "\n",
            "2020-06-08 05:01:57 (35.2 MB/s) - ‘movie_data.csv.gz’ saved [26521894/26521894]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vSOiTn9iHnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "\n",
        "with gzip.open('movie_data.csv.gz') as f_in, open('movie_data.csv', 'wb') as f_out:\n",
        "    f_out.writelines(f_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbnYRNRFiKxY",
        "colab_type": "code",
        "outputId": "96af0a11-a8f6-40fd-f8ff-5ede99fe8b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03WNuYX4iMc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "    return tokenized\n",
        "\n",
        "def stream_docs(path):\n",
        "    with open(path, 'r', encoding='utf-8') as csv:\n",
        "        next(csv) # skip header\n",
        "        for line in csv:\n",
        "            text, label = line[:-3], int(line[-2])\n",
        "            yield text, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBhBIaIjiTt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_minibatch(doc_stream, size):\n",
        "    docs, y = [], []\n",
        "    try:\n",
        "        for _ in range(size):\n",
        "            text, label = next(doc_stream)\n",
        "            docs.append(text)\n",
        "            y.append(label)\n",
        "    except StopIteration:\n",
        "        return None, None\n",
        "    return docs, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9k33IaqiV6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore', \n",
        "                         n_features=2**21,\n",
        "                         preprocessor=None, \n",
        "                         tokenizer=tokenizer)\n",
        "\n",
        "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
        "doc_stream = stream_docs(path='movie_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT5VBXSIiY5K",
        "colab_type": "code",
        "outputId": "b0b2e8e3-d2c4-4827-c1fd-d5864f405e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!pip install pyprind"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyprind\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/30/e76fb0c45da8aef49ea8d2a90d4e7a6877b45894c25f12fb961f009a891e/PyPrind-2.11.2-py3-none-any.whl\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I-7Ep_Ria-r",
        "colab_type": "code",
        "outputId": "9fc44ab0-1a50-460c-8a5f-ce4449776484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pyprind\n",
        "pbar = pyprind.ProgBar(45)\n",
        "\n",
        "classes = np.array([0, 1])\n",
        "for _ in range(45):\n",
        "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
        "    if not X_train:\n",
        "        break\n",
        "    X_train = vect.transform(X_train)\n",
        "    clf.partial_fit(X_train, y_train, classes=classes)\n",
        "    pbar.update()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:27\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G9mIYa7id5d",
        "colab_type": "code",
        "outputId": "9f57f884-02cf-4d3a-a5da-c6c5f1ac3755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
        "X_test = vect.transform(X_test)\n",
        "print('정확도: %.3f' % clf.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정확도: 0.868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-xU18C7ifkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = clf.partial_fit(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeCilkXozUhw",
        "colab_type": "text"
      },
      "source": [
        "## 9.1 학습된 사이킷런 추정기 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isb8yEsQdY-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
        "\n",
        "if not os.path.exists(dest):\n",
        "    os.makedirs(dest) # 데이터를 저장할 디렉터리 생성\n",
        "\n",
        "pickle.dump(stop, # 자연어 라이브러리의 불용어 저장 기능\n",
        "            open(os.path.join(dest, 'stopwords.pkl'), 'wb'), # wb: 이진 모드로 파일 오픈\n",
        "            protocol = 4) # 파이썬 3.4 버전에 최적화\n",
        "\n",
        "pickle.dump(clf, \n",
        "            open(os.path.join(dest, 'classifier.pkl'), 'wb'),\n",
        "            protocol = 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tunVlz0GiAWY",
        "colab_type": "code",
        "outputId": "4e40462d-baca-4bce-a152-f2d1e5ba48d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile movieclassifier/vectorizer.py\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "cur_dir = os.path.dirname(__file__)\n",
        "stop = pickle.load(open(os.path.join(cur_dir,\n",
        "                                     'pkl_object',\n",
        "                                     'stopwords.pkl'), 'rb'))\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emotions = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
        "                          text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
        "                   + ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# 토큰형 행렬로 문서를 변환해주는 메서드\n",
        "vect = HashingVectorizer(decode_error = 'ignore',\n",
        "                         n_features = 2 * 21,\n",
        "                         preprocessor = None,\n",
        "                         tokenizer = tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing movieclassifier/vectorizer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAt8F5gmoPJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('movieclassifier') # 디렉터리 위치 변경"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiGEEQqeqhXY",
        "colab_type": "code",
        "outputId": "b8b677b1-93e6-4461-83e3-c2a125413923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "label = {0: '음성', 1: '양성'}\n",
        "\n",
        "example = ['I love this movie']\n",
        "\n",
        "X = vect.transform(example) # HashingVectorizer를 사용해 example을 벡터로 변환 \n",
        "print('예측: %s\\n 확률: %.2f%%' %\\\n",
        "      (label[clf.predict(X)[0]], # 클래스 레이블 예측\n",
        "       np.max(clf.predict_proba(X))*100)) # 예측의 확률값 계산"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "예측: 양성\n",
            " 확률: 81.44%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Am8GJTzCsH",
        "colab_type": "text"
      },
      "source": [
        "## 9.2 데이터를 저장하기 위해 SQLite 데이터베이스 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W3FkUbQzBtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('reviews.sqlite') # SQLite 데이터베이스 파일에 연결\n",
        "c = conn.cursor() # 데이터베이스 커서 생성 (DB 문법 사용 가능)\n",
        "\n",
        "c.execute('DROP TABLE IF EXISTS review_db')\n",
        "c.execute('CREATE TABLE review_db'\\\n",
        "          '(review TEXT, sentiment INTEGER, date TEXT)') # 컬럼 생성\n",
        "\n",
        "example1 = 'I love this movie'\n",
        "c.execute('INSERT INTO review_db'\\\n",
        "          \"(review, sentiment, date) VALUES\"\\\n",
        "          \"(?, ?, DATETIME('now'))\", (example1, 1))\n",
        "\n",
        "example2 = 'I dislike this movie'\n",
        "c.execute(\"INSERT INTO review_db\"\\\n",
        "          \"(review, sentiment, date) VALUES\"\\\n",
        "          \"(?, ?, DATETIME('now'))\", (example2, 0))\n",
        "\n",
        "conn.commit() # 정상적으로 실행시 변경사항 저장\n",
        "conn.close() # 종료"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nWpYH8_2B9K",
        "colab_type": "code",
        "outputId": "d70a67aa-f626-43ce-ee58-ea364c0d97a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "conn = sqlite3.connect('reviews.sqlite') # 재연결\n",
        "c = conn.cursor()\n",
        "c.execute(\"SELECT * FROM review_db WHERE date\"\\\n",
        "          \" BETWEEN '2017-01-01 00:00:00' AND DATETIME('now')\") # 2017년부터 오늘까지 추가된 모든 열 추출\n",
        "\n",
        "results = c.fetchall()\n",
        "conn.close()\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I love this movie', 1, '2020-06-08 05:05:17'), ('I dislike this movie', 0, '2020-06-08 05:05:17')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
