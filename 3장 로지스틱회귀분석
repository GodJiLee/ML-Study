# 3장. 사이킷런을 타고 떠나는 머신 러닝 분류 모델 투어

# 로지스틱 회귀를 사용한 클래스 확률 모델링

## 로지스틱 회귀의 이해와 조건부 확률


```python
import matplotlib.pyplot as plt
import numpy as np

# 시그모이드 함수의 모습을 -7에서 7까지 시각화 
def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

z = np.arange(-7, 7, 0.1)
phi_z = sigmoid(z)

plt.plot(z, phi_z)
plt.axvline(0.0, color = 'k')
plt.ylim(-0.1, 1.1)
plt.xlabel('z')
plt.ylabel('$\phi (z)$')

# y축의 눈금과 격자선
plt.yticks([0.0, 0.5, 1.0])
ax = plt.gca()
ax.yaxis.grid(True)

plt.tight_layout()
plt.show()

# 0.5에서 중간값을 가지고 z값이 커질 수록 1에, 작아질 수록 0에 가까워지는 s자 형의 그래프
# 0~1 사이의 값을 가지므로 확률로써 적용할 수 있음
```


![png](output_3_0.png)


## 로지스틱 비용 함수의 가중치 학습하기


```python
def cost_1(z):
    return -np.log(sigmoid(z)) # y = 1일 때 비용함수 

def cost_0(z):
    return -np.log(1 - sigmoid(z)) # y = 0일 때 비용함수 

z = np.arange(-10, 10, 0.1)
phi_z = sigmoid(z)

c1 = [cost_1(x) for x in z]
plt.plot(phi_z, c1, label = 'J(w) if y = 1')

c0 = [cost_0(x) for x in z]
plt.plot(phi_z, c0, linestyle = '--', label = 'J(w) if y = 0')

# 샘플이 1개일 경우 분류 비용을 시각화 
plt.ylim(0.0, 5.1)
plt.xlim([0,1])
plt.xlabel('$\phi$(z)')
plt.ylabel('J(w)')
plt.legend(loc = 'best')
plt.tight_layout()
plt.show()

# y = 1일 때의 J(w)는 phi(z)->1일 수록 비용 최소화ㅜ
# y = 0일 때의 J(w)는 phi(z)->0일 수록 비용 최소화
# 즉, 정확히 예측할 수록 비용이 줄어드는 반면, 잘못 예측할 수록 비용이 크게 증가하는 양상
```


![png](output_5_0.png)


## 아달린 구현을 로지스틱 회귀 알고리즘으로 변경


```python
class LogisticRegressionGD(object):
    """경사 하강법을 사용한 로지스틱 회귀 분류기
    매개변수
    -------------------
    eta : float
        학습률(0.0과 1.0 사이)
    n_iter : int
        훈련 데이터셋 반복 횟수
    random_state = int
        가중치 무작위 초기화를 위한 난수 생성기 시드
        
    속성
    -------------------
    w_ : 1d-array
        학습된 가중치
    cost_ : list
        에포크마다 누적된 로지스틱 비용 함수 값
    """

    # 매개변수 초기화 
    def __init__(self, eta = 0.05, n_iter = 100, random_state = 1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
        
    def fit(self, X, y):
        """ 훈련 데이터 학습
        
        매개변수
        ------------------
        X : {array-like}, shape = [n_sample, n_feature]
            n_samples개의 샘플과 n_features개의 특성으로 이루어진 훈련 데이터
        y : array-like, shape = [n_samples]
            타깃값
            
        반환값
        ------------------
        self : object
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc = 0.0, scale = 0.01, size = 1 + X.shape[1])
        self.cost_ = []
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # 오차 제곱합 대신 로지스틱 비용을 계산
            cost = -y.dot(np.log(output)) - ((1 - y).dot(np.log(1 - output)))
            self.cost_.append(cost)
        return self

    def net_input(self, X):
        """최종 입력 계산"""
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def activation(self, z):
        """로지스틱 시그모이드 활성화 계산"""
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))

    def predict(self, X):
        """단위 계단 함수를 사용하여 클래스 레이블을 반환합니다"""
        return np.where(self.net_input(X) >= 0.0, 1, 0)
        # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0) 와 동일 
        
# 아달린 구현을 로지스틱 회귀 알고리즘으로 변경한 것으로 기존 항등함수(선형함수)였던
# activation function을 sigmoid 함수로 변형
```


```python
X_train_01_subset = X_train[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]

# 로지스틱 회귀 + SGD로 분류
lrgd = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1)
lrgd.fit(X_train_01_subset,
         y_train_01_subset)

# 결정경계 그리기
plot_decision_regions(X=X_train_01_subset, 
                      y=y_train_01_subset,
                      classifier=lrgd)

#시각화
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()

# 로지스틱 회귀 방식으로
# petal 너비와 길이에 따라 레이블 0,1을 가지는 species를 분류
```


![png](output_8_0.png)


## 사이킷런을 사용해 로지스틱 회귀 모델 훈련하기


```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(solver = 'liblinear', multi_class = 'auto', C = 100.0, random_state = 1)
lr.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined, 
                     classifier = lr, test_idx = range(105, 150))

plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc = 'upper left')
plt.tight_layout()
plt.show()

# 표준화된 훈련 데이터셋에 대해 로지스틱 회귀 분류 모델로 경계를 나눈 결과
# 이전과 달리 다중 분류도 지원
# 여기서 C란 규제의 강도를 조정할 수 있는 매개변수 
```


![png](output_10_0.png)



```python
# predict_proba 메서드를 사용해 훈련 샘플이 특정 클래스에 속할 확률을 계산
# 결과 배열에서 각 행은 각 species의 클래스 소속 확률
# 1행은 3번째 클래스에 속할 확률이 가장 높음 
lr.predict_proba(X_test_std[:3, :])
```




    array([[3.17983737e-08, 1.44886616e-01, 8.55113353e-01],
           [8.33962295e-01, 1.66037705e-01, 4.55557009e-12],
           [8.48762934e-01, 1.51237066e-01, 4.63166788e-13]])




```python
# 확률이므로 열을 모두 더하면 1이 됨
lr.predict_proba(X_test_std[:3, :]).sum(axis = 1)
```




    array([1., 1., 1.])




```python
# argmax 함수를 사용해 행 중 최댓값을 예측 레이블로 할당
lr.predict_proba(X_test_std[:3, :]).argmax(axis = 1)
```




    array([2, 0, 0], dtype=int64)




```python
#predict 메서드로 더 빠르게 확인 가능
lr.predict(X_test_std[:3, :])
```




    array([2, 0, 0])




```python
# 입력데이터의 차원을 2차원으로 맞춰주기 위해 형상 변환
lr.predict(X_test_std[0, :].reshape(1, -1))
```




    array([2])



## 규제를 사용해 과대적합 피하기 


```python
weights, params = [], []

# 10^-5 ~ 10^5까지 C를 조정
for c in np.arange(-5, 5):
    lr = LogisticRegression(solver = 'liblinear', multi_class = 'auto', C = 10.**c, random_state = 1)
    lr.fit(X_train_std, y_train)
    weights.append(lr.coef_[1])
    params.append(10.**c)
    
weights = np.array(weights)
plt.plot(params, weights[:, 0],
        label = 'petal length')
plt.plot(params, weights[:, 1], linestyle = '--',
        label = 'petal width')
plt.ylabel('weight coefficient')
plt.xlabel('C')
plt.legend(loc = 'upper left')
plt.xscale('log')
plt.show()

# C가 줄어들 수록 규제강도가 증가하여 가중치 절댓값이 작아짐 (overfitting 억제)
# 이때 C는 L2 규제에서 규제의 강도를 조정하는 하이퍼파라미터 람다의 역수
```


![png](output_17_0.png)


## 서포트 벡터 머신을 사용한 최대 마진 분류


```python
from sklearn.svm import SVC #SVM 모델을 훈련하기 위한 클래스

# 서포트 벡터 머신을 활용한 분류
svm = SVC(kernel = 'linear', C = 1.0, random_state = 1)
svm.fit(X_train_std, y_train) #훈련 데이터에 svm 적용

plot_decision_regions(X_combined_std, #결정경계 그리기
                     y_combined,
                     classifier = svm,
                     test_idx = range(105, 150))

#시각화
plt.scatter(svm.dual_coef_[0, :], svm.dual_coef_[1, :])
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc = 'upper left')
plt.tight_layout()
plt.show()
```


![png](output_19_0.png)



```python
svm.coef_ # 공분산 배열
```




    array([[-1.28108835, -1.08908074],
           [-0.68216416, -0.56900015],
           [-2.20580665, -1.9370517 ]])




```python
svm.dual_coef_, svm.dual_coef_.shape
```




    (array([[ 0.88134491,  0.60379849, -1.        , -0.        , -0.        ,
             -0.        , -0.        , -0.4851434 , -0.        , -0.        ,
             -0.        , -0.        , -0.        , -0.        , -0.        ,
             -0.        , -0.        , -0.        , -0.        , -0.        ,
             -0.        , -0.        , -0.        , -0.        , -0.        ,
             -0.39455455],
            [ 0.39455455,  0.        ,  0.        ,  1.        ,  0.19375927,
              1.        ,  1.        ,  0.        ,  1.        ,  1.        ,
              1.        ,  1.        ,  1.        ,  1.        ,  1.        ,
             -1.        , -1.        , -1.        , -1.        , -1.        ,
             -1.        , -1.        , -1.        , -0.19375927, -1.        ,
             -1.        ]]),
     (2, 26))



### 사이킷런의 다른 구현


```python
from sklearn.linear_model import SGDClassifier #SVC 대신 확률적 경사 하강법(SGD)를 적용한 분류기

ppn = SGDClassifier(loss = 'perceptron') #퍼셉트론 분류기
lr = SGDClassifier(loss = 'log') 
svm = SGDClassifier(loss = 'hinge')
```

## 커널 SVM을 사용해 비선형 문제 풀기 


```python
import matplotlib.pyplot as plt
import numpy as np

#선형적으로 분류할 수 없는 데이터셋
np.random.seed(1)
X_xor = np.random.randn(200, 2) #200개의 데이터를 2개의 클래스로 분류
y_xor = np.logical_xor(X_xor[:, 0] > 0, #XOR(NAND + OR)
                       X_xor[:, 1] > 0)
y_xor = np.where(y_xor, 1, -1)

#산점도 그리기
plt.scatter(X_xor[y_xor == 1, 0],
            X_xor[y_xor == 1, 1],
            c='b', marker='x',
            label='1')
plt.scatter(X_xor[y_xor == -1, 0],
            X_xor[y_xor == -1, 1],
            c='r',
            marker='s',
            label='-1')

plt.xlim([-3, 3])
plt.ylim([-3, 3])
plt.legend(loc='best')
plt.tight_layout()
plt.show()
```


![png](output_25_0.png)


### 커널 기법을 사용해 고차원 공간에서 분할 초평면 찾기


```python
#고차원으로 투영 후 선형적으로 구분되도록 하는 커널 방식 적용
svm = SVC(kernel = 'rbf', random_state = 1, gamma = 0.10, C = 10.0) #방사기저함수 방식의 커널 사용
svm.fit(X_xor, y_xor)

#결정경계 그리기
plot_decision_regions(X_xor, y_xor, classifier = svm) #서포트벡터머신을 분류기로 사용

plt.legend(loc = 'upper left')
plt.tight_layout()
plt.show()
```


![png](output_27_0.png)



```python
svm = SVC(kernel = 'rbf', random_state = 1, gamma = 0.2, C = 1.0) #감마는 결정경계를 더 세밀하게 만들어주는 매개변수 
                                                                    #여기서는 0.2로 비교적 작은 값 선택
svm.fit(X_train_std, y_train) #훈련 데이터셋에 대해 rbf 커널 SVM

#결정 경계 그리기
plot_decision_regions(X_combined_std, y_combined,
                      classifier = svm, test_idx = range(105, 150))
plt.scatter(svm.dual_coef_[0,:], svm.dual_coef_[1,:])
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc = 'upper left')
plt.tight_layout()
plt.show()

# 비교적 부드러운 형태의 결정경계
```


![png](output_28_0.png)

